{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPyBvYRHAUCxbvgfwiehRXO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zachary013/Lab3-DL/blob/main/Atelier3_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part1**"
      ],
      "metadata": {
        "id": "LXOInZD2prlb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQjLsTnvhxeE",
        "outputId": "d5b53b2c-154c-4599-eace-0fbb3535407f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n",
            "GPU device name: Tesla T4\n",
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.43.4)\n",
            "Collecting Twisted>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.2)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from farasapy) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from farasapy) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2025.3.2)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.2)\n",
            "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading automat-25.4.16-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface>=5.1.0->scrapy) (75.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (2025.1.31)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.8.0-py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.2.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading automat-25.4.16-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: PyDispatcher, zope.interface, w3lib, queuelib, pyarabic, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, farasapy, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-25.4.16 constantly-23.10.4 cssselect-1.3.0 farasapy-0.0.14 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.10.0 protego-0.4.0 pyarabic-0.6.15 queuelib-1.8.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 tldextract-5.2.0 w3lib-2.3.1 zope.interface-7.2\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
        "\n",
        "# Install required libraries\n",
        "!pip install scrapy beautifulsoup4 pyarabic farasapy transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Web scraping with BeautifulSoup\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "\n",
        "# List of Arabic news websites\n",
        "websites = [\n",
        "    \"https://www.aljazeera.net/news/politics\",\n",
        "    \"https://www.bbc.com/arabic\",\n",
        "    \"https://arabic.cnn.com/\",\n",
        "    # Add more sites as needed\n",
        "]\n",
        "\n",
        "collected_texts = []\n",
        "\n",
        "# Scrape text from each website\n",
        "for website in websites[:2]:  # Limiting to first 2 for demonstration\n",
        "    try:\n",
        "        response = requests.get(website, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Extract paragraphs (adjust selectors based on website structure)\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "\n",
        "        for p in paragraphs:\n",
        "            text = p.get_text().strip()\n",
        "            # Filter for Arabic text with decent length\n",
        "            if len(text) > 100 and re.search(r'[\\u0600-\\u06FF]', text):\n",
        "                collected_texts.append(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {website}: {e}\")\n",
        "\n",
        "# Create a dataset with assigned scores (simulated for this example)\n",
        "data = []\n",
        "for text in collected_texts[:20]:  # Limiting to first 20 texts\n",
        "    # Assign a random score between 0 and 10 for demonstration\n",
        "    score = round(random.uniform(0, 10), 1)\n",
        "    data.append({\"text\": text, \"score\": score})\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"Collected {len(df)} texts\")\n",
        "df.head()\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('arabic_texts.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lxinf3viRk5",
        "outputId": "d4b09c33-e5e8-471a-f7dc-5046f23441c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 20 texts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP Preprocessing Pipeline\n",
        "import re\n",
        "import numpy as np\n",
        "from pyarabic import araby\n",
        "from farasa.stemmer import FarasaStemmer\n",
        "\n",
        "# Initialize Farasa stemmer\n",
        "stemmer = FarasaStemmer()\n",
        "\n",
        "def preprocess_arabic_text(text):\n",
        "    # Remove diacritics\n",
        "    text = araby.strip_tashkeel(text)\n",
        "\n",
        "    # Remove non-Arabic characters and extra spaces\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = araby.tokenize(text)\n",
        "\n",
        "    # Stemming\n",
        "    stems = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Remove Arabic stop words (you might need a comprehensive list)\n",
        "    arabic_stop_words = set(['من', 'إلى', 'في', 'على', 'عن', 'مع', 'هذا', 'هذه', 'ذلك', 'تلك'])\n",
        "    filtered_stems = [token for token in stems if token not in arabic_stop_words]\n",
        "\n",
        "    return \" \".join(filtered_stems)\n",
        "\n",
        "# Apply preprocessing to all texts\n",
        "df['processed_text'] = df['text'].apply(preprocess_arabic_text)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_amXGqNkKSX",
        "outputId": "5a00c081-37d9-4429-b8ba-3141497f2c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'farasa-api.qcri.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 241M/241M [08:33<00:00, 470kiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for deep learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Tokenize and convert to indices\n",
        "from collections import Counter\n",
        "\n",
        "# Create vocabulary from processed texts\n",
        "all_words = ' '.join(df['processed_text']).split()\n",
        "word_counts = Counter(all_words)\n",
        "vocabulary = {word: idx + 1 for idx, (word, _) in enumerate(word_counts.most_common())}\n",
        "vocabulary['<PAD>'] = 0\n",
        "\n",
        "# Convert texts to sequences of indices\n",
        "def text_to_sequence(text, max_length=100):\n",
        "    words = text.split()\n",
        "    sequence = [vocabulary.get(word, 0) for word in words[:max_length]]\n",
        "    if len(sequence) < max_length:\n",
        "        sequence += [0] * (max_length - len(sequence))\n",
        "    return sequence\n",
        "\n",
        "df['sequence'] = df['processed_text'].apply(text_to_sequence)\n",
        "\n",
        "# Normalize scores to [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "df['normalized_score'] = scaler.fit_transform(df[['score']])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    list(df['sequence']),\n",
        "    list(df['normalized_score']),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create dataset and dataloader\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
        "        self.labels = [torch.tensor([label], dtype=torch.float) for label in labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "m2FXk0KjlBfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model architectures\n",
        "\n",
        "# 1. Simple RNN Model\n",
        "class SimpleRNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(SimpleRNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "# 2. Bidirectional RNN\n",
        "class BiRNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiRNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        # Concatenate final forward and backward hidden states\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        return self.fc(hidden)\n",
        "\n",
        "# 3. LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "# 4. GRU Model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "metadata": {
        "id": "gi45i-JcnFjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation function\n",
        "def train_model(model, train_loader, test_loader, optimizer, criterion, epochs=10):\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch_idx, (texts, labels) in enumerate(train_loader):\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(texts)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        predictions_list = []\n",
        "        actual_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for texts, labels in test_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                predictions = model(texts)\n",
        "                loss = criterion(predictions, labels)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                predictions_list.extend(predictions.cpu().numpy())\n",
        "                actual_list.extend(labels.cpu().numpy())\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(f'Epoch: {epoch+1}')\n",
        "        print(f'Train Loss: {train_loss:.4f}')\n",
        "        print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "        # Convert back to original scale for MSE calculation\n",
        "        pred_orig = scaler.inverse_transform(np.array(predictions_list))\n",
        "        actual_orig = scaler.inverse_transform(np.array(actual_list))\n",
        "        mse = np.mean((pred_orig - actual_orig) ** 2)\n",
        "        print(f'Test MSE (original scale): {mse:.4f}')\n",
        "        print('-' * 60)\n",
        "\n",
        "    return train_losses, test_losses\n",
        "\n",
        "# Initialize model parameters\n",
        "vocab_size = len(vocabulary)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 1\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Train each model\n",
        "models = {\n",
        "    \"Simple RNN\": SimpleRNNModel(vocab_size, embedding_dim, hidden_dim, output_dim),\n",
        "    \"Bidirectional RNN\": BiRNNModel(vocab_size, embedding_dim, hidden_dim, output_dim),\n",
        "    \"LSTM\": LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim),\n",
        "    \"GRU\": GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name} model...\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    train_losses, test_losses = train_model(\n",
        "        model, train_loader, test_loader, optimizer, criterion, epochs\n",
        "    )\n",
        "\n",
        "    results[name] = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"model\": model\n",
        "    }"
      ],
      "metadata": {
        "id": "-9VVsvWJnIjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation and metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in test_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            output = model(texts)\n",
        "\n",
        "            predictions.extend(output.cpu().numpy())\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Convert back to original scale\n",
        "    predictions = scaler.inverse_transform(np.array(predictions))\n",
        "    actuals = scaler.inverse_transform(np.array(actuals))\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    r2 = r2_score(actuals, predictions)\n",
        "\n",
        "    return {\n",
        "        \"MSE\": mse,\n",
        "        \"MAE\": mae,\n",
        "        \"R2\": r2\n",
        "    }\n",
        "\n",
        "# Evaluate each model\n",
        "evaluation_results = {}\n",
        "\n",
        "for name, result in results.items():\n",
        "    model = result[\"model\"]\n",
        "    metrics = evaluate_model(model, test_loader)\n",
        "    evaluation_results[name] = metrics\n",
        "\n",
        "    print(f\"Evaluation for {name}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Plot loss curves\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "for i, (name, result) in enumerate(results.items()):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.plot(result[\"train_losses\"], label=\"Train Loss\")\n",
        "    plt.plot(result[\"test_losses\"], label=\"Test Loss\")\n",
        "    plt.title(f\"{name} Loss Curves\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y3PcRhaKnLbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part2"
      ],
      "metadata": {
        "id": "8ty3IvBKnTlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "57zKAteGnaHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset for fine-tuning\n",
        "import os\n",
        "\n",
        "# Function to create a simple dataset\n",
        "def create_custom_dataset(filename=\"custom_dataset.txt\"):\n",
        "    # Create a simple dataset with some text entries\n",
        "    # You can replace this with your own dataset creation logic\n",
        "    text_samples = [\n",
        "        \"مرحبا بك في عالم الذكاء الاصطناعي.\",\n",
        "        \"تعلم الآلة هو مجال فرعي من الذكاء الاصطناعي.\",\n",
        "        \"نماذج اللغة الكبيرة أحدثت ثورة في مجال معالجة اللغة الطبيعية.\",\n",
        "        \"الشبكات العصبية العميقة قادرة على التعلم من البيانات.\",\n",
        "        \"تستخدم خوارزميات التعلم العميق في العديد من التطبيقات.\",\n",
        "        # Add more sentences as needed\n",
        "    ]\n",
        "\n",
        "    # Write samples to a file\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sample in text_samples:\n",
        "            f.write(sample + \"\\n\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "# Create dataset file\n",
        "dataset_file = create_custom_dataset()\n",
        "print(f\"Dataset created at: {dataset_file}\")"
      ],
      "metadata": {
        "id": "EumdtYbYnjbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"  # Can also use \"gpt2-medium\", \"gpt2-large\" or \"gpt2-xl\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token to GPT2 tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "KxZT_V-Unk_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for fine-tuning\n",
        "def load_dataset(train_path, tokenizer, block_size=128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = load_dataset(dataset_file, tokenizer)\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")"
      ],
      "metadata": {
        "id": "SPq67Cl_nmwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model_path = \"./fine_tuned_gpt2\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "metadata": {
        "id": "ijNq5zeKno7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text with the fine-tuned model\n",
        "def generate_text(prompt, model, tokenizer, max_length=100):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        top_k=50,\n",
        "        temperature=0.7,\n",
        "        no_repeat_ngram_size=2,\n",
        "    )\n",
        "\n",
        "    # Decode and return the text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
        "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Generate text\n",
        "test_prompt = \"مرحبا بك في\"\n",
        "generated_text = generate_text(test_prompt, fine_tuned_model, fine_tuned_tokenizer)\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(f\"Generated text: {generated_text}\")"
      ],
      "metadata": {
        "id": "pvauVVRRnqix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the text generation\n",
        "def evaluate_generation(model, tokenizer, prompts):\n",
        "    results = []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        generated = generate_text(prompt, model, tokenizer)\n",
        "        results.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"generated\": generated\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"الذكاء الاصطناعي هو\",\n",
        "    \"تعلم الآلة يساعدنا في\",\n",
        "    \"اللغة العربية لها\"\n",
        "]\n",
        "\n",
        "# Run evaluation\n",
        "evaluation = evaluate_generation(fine_tuned_model, fine_tuned_tokenizer, test_prompts)\n",
        "\n",
        "# Print results\n",
        "for result in evaluation:\n",
        "    print(f\"Prompt: {result['prompt']}\")\n",
        "    print(f\"Generated: {result['generated']}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "EV02Gqwlnt7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model and results\n",
        "import json\n",
        "\n",
        "# Save evaluation results\n",
        "with open(\"text_generation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(evaluation, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Save a sample of the generated text\n",
        "with open(\"sample_generated_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for result in evaluation:\n",
        "        f.write(f\"Prompt: {result['prompt']}\\n\")\n",
        "        f.write(f\"Generated: {result['generated']}\\n\")\n",
        "        f.write(\"-\" * 60 + \"\\n\")\n",
        "\n",
        "# Export conclusion\n",
        "conclusion = \"\"\"\n",
        "Lab 3 Summary:\n",
        "1. Created an Arabic text classifier using various recurrent neural architectures (RNN, BiRNN, LSTM, GRU)\n",
        "2. Fine-tuned a GPT-2 model for Arabic text generation\n",
        "3. Compared the performance of different sequence models on the classification task\n",
        "4. Successfully generated coherent Arabic text continuations\n",
        "\n",
        "The most efficient model for the classification task was [insert best model here] based on the evaluation metrics.\n",
        "For text generation, the fine-tuned GPT-2 model produced [describe quality] results.\n",
        "\n",
        "Key learnings:\n",
        "- Sequence models are effective for both classification and generation tasks\n",
        "- Bidirectional models often capture more context than unidirectional ones\n",
        "- Pre-trained transformer models can be fine-tuned effectively even with small datasets\n",
        "\"\"\"\n",
        "\n",
        "with open(\"lab3_conclusion.txt\", \"w\") as f:\n",
        "    f.write(conclusion)\n",
        "\n",
        "print(\"Lab 3 completed successfully!\")"
      ],
      "metadata": {
        "id": "B8w5POXYnwPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}