# ğŸ§  Deep Learning Lab 3: Arabic NLP & Text Generation ğŸš€

![Deep Learning](https://img.shields.io/badge/Deep_Learning-NLP-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-1.10+-orange)
![Transformers](https://img.shields.io/badge/Transformers-4.20+-green)
![GPU](https://img.shields.io/badge/GPU-Enabled-brightgreen)
![Arabic NLP](https://img.shields.io/badge/Arabic-NLP-yellow)

## ğŸ“š Table of Contents
- [Overview](#-overview)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Data Collection & Preprocessing](#-data-collection--preprocessing)
- [Classification Models](#-classification-models)
- [Transformer Text Generation](#-transformer-text-generation)
- [Evaluation Metrics](#-evaluation-metrics)
- [Results & Analysis](#-results--analysis)
- [Key Learnings](#-key-learnings)
- [Future Improvements](#-future-improvements)
- [References](#-references)

## ğŸ” Overview
This project implements advanced deep learning architectures for Natural Language Processing tasks focusing on the Arabic language. It consists of two main components:

1. **Arabic Text Classification**: Building and comparing various recurrent neural network architectures (RNN, Bidirectional RNN, GRU, and LSTM) to classify Arabic texts based on their relevance scores.
   
2. **Arabic Text Generation**: Fine-tuning a GPT-2 model to generate coherent Arabic text continuations from given prompts.

This implementation is part of the Deep Learning course (Master MBD) supervised by Prof. ELAACHAk LOTFI at UniversitÃ© Abdelmalek Essaadi.

<p align="center">
  <img src="https://miro.medium.com/max/1400/1*uCMelgC_N3Q8VzI_3QiQqQ.png" width="600" alt="NLP Deep Learning Architecture">
</p>

## ğŸ“‚ Project Structure
```
deep-learning-lab3/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ part1_arabic_classification.ipynb
â”‚   â””â”€â”€ part2_gpt2_finetuning.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ arabic_texts_raw.csv
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ arabic_texts_processed.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ classification/
â”‚   â”‚   â”œâ”€â”€ rnn_model.pth
â”‚   â”‚   â”œâ”€â”€ birnn_model.pth
â”‚   â”‚   â”œâ”€â”€ lstm_model.pth
â”‚   â”‚   â””â”€â”€ gru_model.pth
â”‚   â””â”€â”€ generation/
â”‚       â””â”€â”€ fine_tuned_gpt2/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scraping.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ train_models.py
â”‚   â””â”€â”€ generate_text.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ model_comparison.csv
â”‚   â”œâ”€â”€ learning_curves.png
â”‚   â””â”€â”€ generated_samples.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## ğŸ›  Installation
To run this project, you need to install the required dependencies. You can do this by running:

```bash
pip install -r requirements.txt
```

Or directly in Google Colab:

```python
!pip install torch transformers datasets scrapy beautifulsoup4 pyarabic farasapy matplotlib
```

### ğŸ’» Setup Google Colab with GPU
1. Open [Google Colab](https://colab.research.google.com/)
2. Create a new notebook
3. Go to Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU
4. Verify GPU is available:
   ```python
   import torch
   print(f"GPU available: {torch.cuda.is_available()}")
   print(f"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
   ```

## ğŸ“Š Data Collection & Preprocessing

### Web Scraping
Arabic text data is collected from various news websites and blogs using BeautifulSoup and Scrapy. Each text is assigned a relevance score between 0 and 10.

```python
# Web scraping with BeautifulSoup
import requests
from bs4 import BeautifulSoup

websites = ["https://www.aljazeera.net", "https://www.bbc.com/arabic", ...]

# Extract Arabic text content
# Score assignment
```

### Text Preprocessing Pipeline
The collected Arabic texts undergo a comprehensive preprocessing pipeline:

1. **Diacritics Removal**: Remove Arabic diacritical marks (ØªØ´ÙƒÙŠÙ„)
2. **Tokenization**: Split text into individual tokens
3. **Stemming**: Extract word stems using Farasa stemmer
4. **Stop Words Removal**: Remove common Arabic stop words
5. **Normalization**: Standardize text (e.g., normalize various forms of Alif)

<p align="center">
  <img src="https://miro.medium.com/max/1400/1*Zdxav15O9xVh7GQNaf5FJQ.png" width="650" alt="NLP Preprocessing Pipeline">
</p>

## ğŸ§® Classification Models
We implement four different sequence model architectures for the classification task:

### 1. Simple RNN
```python
class SimpleRNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(SimpleRNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, text):
        embedded = self.embedding(text)
        output, hidden = self.rnn(embedded)
        return self.fc(hidden.squeeze(0))
```

### 2. Bidirectional RNN
Processes text in both directions to capture context from past and future tokens.

### 3. LSTM (Long Short-Term Memory)
Addresses the vanishing gradient problem with memory cells and gating mechanisms.

### 4. GRU (Gated Recurrent Unit)
A simplified variant of LSTM with fewer parameters but comparable performance.

## ğŸ¤– Transformer Text Generation
The second part of the project focuses on fine-tuning a pre-trained GPT-2 model for Arabic text generation.

### Model Architecture
GPT-2 is a transformer-based language model with a decoder-only architecture:

<p align="center">
  <img src="https://miro.medium.com/max/1400/1*iJcUH1F0TmCQE9yCrJjvxg.png" width="500" alt="GPT-2 Architecture">
</p>

### Fine-tuning Process
1. Load the pre-trained GPT-2 model
2. Create a custom Arabic text dataset
3. Fine-tune the model on this dataset
4. Generate text based on input prompts

```python
# Example text generation function
def generate_text(prompt, model, tokenizer, max_length=100):
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    
    output = model.generate(
        input_ids,
        max_length=max_length,
        num_return_sequences=1,
        do_sample=True,
        top_p=0.95,
        top_k=50,
        temperature=0.7,
    )
    
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    return generated_text
```

## ğŸ“ Evaluation Metrics

### Classification Metrics
- **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual scores
- **Mean Absolute Error (MAE)**: Measures the average absolute difference between predictions and actual values
- **RÂ² Score**: Indicates the proportion of variance in the dependent variable that's predictable from the independent variables

### Text Generation Metrics
- **BLEU Score**: Evaluates the quality of generated text by comparing with reference texts
- **Perplexity**: Measures how well a probability model predicts a sample
- **Human Evaluation**: Subjective assessment of fluency, coherence, and relevance

## ğŸ“ˆ Results & Analysis

### Classification Models Comparison

| Model | MSE | MAE | RÂ² | Training Time |
|-------|-----|-----|-----|--------------|
| RNN   | 0.45| 0.52| 0.63| 3m 20s       |
| BiRNN | 0.38| 0.47| 0.71| 4m 15s       |
| LSTM  | 0.32| 0.43| 0.75| 5m 40s       |
| GRU   | 0.34| 0.45| 0.73| 4m 50s       |

<p align="center">
  <img src="https://miro.medium.com/max/1400/1*U3Qa3gWrPfGAMMrJdsvj8Q.png" width="600" alt="Learning Curves">
</p>

### Text Generation Examples

**Prompt**: "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ"

**Generated**: "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ Ù…Ø¬Ø§Ù„ Ù…Ù† Ù…Ø¬Ø§Ù„Ø§Øª Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø§Ù„Ø°ÙŠ ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø£Ù†Ø¸Ù…Ø© Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ù…Ù‡Ø§Ù… ØªØªØ·Ù„Ø¨ Ø¹Ø§Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¨Ø´Ø±ÙŠ. ÙŠØ´Ù…Ù„ Ø°Ù„Ùƒ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙˆØ­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø¥Ø¯Ø±Ø§Ùƒ Ø§Ù„Ø¨ØµØ±ÙŠ ÙˆÙÙ‡Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©."

**Prompt**: "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù‡Ø§"

**Generated**: "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù‡Ø§ ØªØ§Ø±ÙŠØ® ØºÙ†ÙŠ ÙˆØ¹Ø±ÙŠÙ‚ ÙŠÙ…ØªØ¯ Ù„Ø£ÙƒØ«Ø± Ù…Ù† 1500 Ø¹Ø§Ù…. ØªØªÙ…ÙŠØ² Ø¨Ø«Ø±Ø§Ø¦Ù‡Ø§ Ø§Ù„Ù„ØºÙˆÙŠ ÙˆØªÙ†ÙˆØ¹ Ù…ÙØ±Ø¯Ø§ØªÙ‡Ø§ ÙˆÙ‚ÙˆØ§Ø¹Ø¯Ù‡Ø§ Ø§Ù„Ù†Ø­ÙˆÙŠØ© Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙˆØ§Ù„Ø¯Ù‚ÙŠÙ‚Ø©. ÙˆÙ‡ÙŠ Ù…Ù† Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø³Ø§Ù…ÙŠØ© Ø§Ù„ØªÙŠ Ø§Ù†ØªØ´Ø±Øª ÙÙŠ Ø´Ø¨Ù‡ Ø§Ù„Ø¬Ø²ÙŠØ±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø«Ù… ØªÙˆØ³Ø¹Øª Ù…Ø¹ Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ù„ØªØµØ¨Ø­ Ù„ØºØ© Ø§Ù„Ø¹Ù„Ù… ÙˆØ§Ù„Ø£Ø¯Ø¨ ÙˆØ§Ù„ÙÙ„Ø³ÙØ© Ø®Ù„Ø§Ù„ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø°Ù‡Ø¨ÙŠ Ù„Ù„Ø­Ø¶Ø§Ø±Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©."

## ğŸ”‘ Key Learnings
- **Architecture Impact**: Bidirectional models consistently outperform unidirectional ones by capturing context from both directions.
- **Memory Mechanisms**: LSTM and GRU models show superior performance in capturing long-range dependencies compared to simple RNNs.
- **Arabic Language Challenges**: Arabic morphological complexity requires specialized preprocessing techniques.
- **Transfer Learning**: Fine-tuning pre-trained transformers is highly effective even with limited training data.
- **Hyperparameter Sensitivity**: Model performance varies significantly with different hyperparameter configurations.

